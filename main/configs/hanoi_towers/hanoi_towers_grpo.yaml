# python train_grpo.py configs/hanoi_towers/hanoi_towers_grpo.yaml

# Model - Use base model or SFT checkpoint
model_name: Qwen/Qwen2.5-0.5B-Instruct  # Smaller model for easier testing

# LoRA
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training
num_epochs: 1
max_steps: 500
batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 0.000005
lr_scheduler_type: constant_with_warmup
warmup_ratio: 0.03
use_liger_kernel: false
optim: adamw_torch_fused
gradient_checkpointing: true

# Output
output_dir: ./results
logging_steps: 5
save_steps: 100

# Data - You need to generate this first!
# Use: python -c "from Enigmata.verifiable_tasks.tasks.hanoi_towers.generator import generate; import pandas as pd; pd.DataFrame(list(generate(count=500, difficulty='medium'))).to_parquet('data/hanoi_towers_train.parquet')"
train_dataset_path: ../data/hanoi_towers_train.parquet
eval_split_ratio: 0.1
eval_strategy: "steps"
eval_steps: 50

# Experiment
experiment_name: hanoi_towers_grpo
reward_fns:
  - "hanoi_towers:verify"

# Snapshots
snapshot_prompts_count: 3
snapshot_every_n_steps: 50
log_completions: true

# GRPO-specific
num_generations: 4
max_new_tokens: 512
temperature: 0.8
beta: 0.1

use_vllm: false  # Set to true for faster generation on GPU
