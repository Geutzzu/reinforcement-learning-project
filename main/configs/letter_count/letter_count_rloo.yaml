# python train_rloo.py configs/letter_count/letter_count_rloo.yaml

# Model - Use SFT checkpoint as base
model_name: /workspace/rl/results/letter_count_sft/0.5B-35steps-only-format/checkpoint-35

# LoRA
use_lora: false

# Training
num_epochs: 1
max_steps: 2000
batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 0.00001
lr_scheduler_type: constant_with_warmup
warmup_ratio: 0.01
use_liger_kernel: true
optim: adamw_8bit
gradient_checkpointing: true


# Output
output_dir: results/letter_count_rloo
logging_steps: 5
save_steps: 100

# Data
train_dataset_path: /workspace/rl/data/rl_letter_count_dataset_very_hard.parquet
eval_split_ratio: 0.00
eval_strategy: "steps"
eval_steps: -1

# Experiments
experiment_name: "letter_count_rloo"
reward_fns: ["letter_count:wrapped"]
snapshot_prompts_count: 0
snapshot_every_n_steps: 100

# Logging
log_completions: true

# RLOO-specific
num_generations: 16
max_new_tokens: 3500
temperature: 1.0
beta: 0.05  # RLOO uses KL penalty by default (unlike GRPO which often uses 0.0)
num_iterations: 1  # Fully online (1 iteration per batch)
epsilon: 0.2  # Clipping epsilon

# vLLM (same as GRPO)
use_vllm: true
vllm_mode: colocate
vllm_gpu_memory_utilization: 0.65
vllm_enable_sleep_mode: false
vllm_max_model_length: 3800
