# Model
model_name: /workspace/rl/models/sft/maze_2/merged

# LoRA
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training
num_epochs: 2
max_steps: -1
batch_size: 8
gradient_accumulation_steps: 4
learning_rate: 0.00001
lr_scheduler_type: cosine
warmup_ratio: 0.05
use_liger_kernel: true

# Output
output_dir: /workspace/rl/results
logging_steps: 10
save_steps: 100

# Data
train_dataset_path: /workspace/rl/data/maze_2.parquet
eval_split_ratio: 0.1

# Experiment
experiment_name: maze_2_grpo
reward_fn: maze_2

# Snapshots
snapshot_prompts_count: 3
snapshot_every_n_steps: 100
log_completions: true

# GRPO-specific
num_generations: 16
max_new_tokens: 256
temperature: 0.7
beta: 0.0

# vLLM acceleration
use_vllm: true
vllm_mode: colocate
vllm_gpu_memory_utilization: 0.6
