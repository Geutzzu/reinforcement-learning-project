# python train_grpo.py configs/maze_2/maze_2_grpo.yaml

# Model - Use SFT checkpoint as base
model_name: Qwen/Qwen3-4B-Instruct-2507  # Or path to SFT checkpoint

# LoRA
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training
num_epochs: 1
max_steps: 1000
batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 0.000005
lr_scheduler_type: constant_with_warmup
warmup_ratio: 0.03
use_liger_kernel: true
optim: adamw_8bit
gradient_checkpointing: true


# Output
output_dir: /workspace/rl/results
logging_steps: 5
save_steps: 100  # Checkpoint every 50 steps

# Data
train_dataset_path: /workspace/rl/data/maze_2_reasoning_new_sample_new_template.parquet
eval_split_ratio: 0.0
eval_strategy: "steps"
eval_steps: 100

# Experiment
experiment_name: maze_2_grpo
reward_fns:
  - "maze_2:verify"

# Snapshots
snapshot_prompts_count: 0
snapshot_every_n_steps: 50
log_completions: true

# GRPO-specific
num_generations: 4
max_new_tokens: 1024
temperature: 1.0
beta: 0.0

use_vllm: true
vllm_mode: colocate
vllm_gpu_memory_utilization: 0.45
vllm_enable_sleep_mode: false
vllm_max_model_length: 1024  # Limit context length (Qwen3 default is 262K!)