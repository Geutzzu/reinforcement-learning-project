model_name: Qwen/Qwen2.5-0.5B-Instruct

use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

num_epochs: 1
max_steps: -1
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 0.00001
max_length: 512
use_liger_kernel: false

output_dir: /workspace/rl/results
logging_steps: 10
save_steps: 100

train_dataset_path: /workspace/rl/data/maze_2.parquet
eval_dataset_path: null
eval_split_ratio: 0.1
eval_strategy: epoch  # Options: no, epoch, steps
eval_steps: null      # Only used if eval_strategy: steps

experiment_name: maze_2_online_dpo
reward_fn: maze_2

snapshot_prompts_count: 3
snapshot_every_n_steps: 100

num_generations: 4
max_new_tokens: 256
temperature: 0.7
beta: 0.1

use_vllm: false
